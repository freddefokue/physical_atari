# Physical Atari

**Physical Atari** is a platform for evaluating reinforcement learning (RL) algorithms.

Many RL algorithms are evaluated primarily with simulations, driven by the ease of running experiments that can be replicated by other researchers. Although this is a successful research approach, it is widely recognized in science and engineering that a simulation can only capture part of the complexity of the real system.  With so much RL research using simulators, there is the danger that improvements observed in a simulator does not translate to their real-world counterparts.  This is especially true for RL research developed with the Atari Learning Environment (ALE), as the corresponding physical systems were not easily accessible.  Some of algorithms developed with the ALE have been deployed to other real-world settings, but many RL algorithms have only been tested in the ALE.

The **Physical Atari** platform provides a software and hardware interface between reinforcement learning agents and a modern version of a physical Atari.  This interface enables the evaluation of RL algorithms developed for the ALE with a real-world instantiation.  The physical platform exposes several timing concerns that are not present in the ALE.  The physical Atari system operates in real-time and is not turn based (it does not wait for an agent action).  Physical systems have non-negligible latency and physical systems have unmodelled dynamics (sensor and actuation noise).  Unlike traditional environments that use pixel-perfect emulators (e.g., ALE), this setup integrates a physical **Atari 2600+** console, a camera-based observation pipeline, and a real-time control system using physical actuators.


This platform provides three contributions for the RL research community.
- A physical platform for evaluating RL algorithms that have been primarily developed with the ALE
- An implementation of a game-independent RL algorithm on this platform that learns under real-time constraints to reliably surpass standard benchmark performance within five hours (1 million frames) on multiple Atari games.
- The platform provides insight into the limitations of our simulators.  Discrepancies in performance between the simulator and reality suggests changes to our simulated environments and the metrics for evaluating RL  algorithms.

---

## Overview

The system consists of three main components:

- **Environment**: A modern [Atari 2600+](https://www.amazon.com/Atari-2600/dp/B0CG7LMFKY) console, outputting real 4:3 video over HDMI.  The console is pin-compatible with original Atari game cartridges and joysticks.
- **Agent**: The learning algorithms and supporting control logic run on a gaming laptop or workstation.
- **Interface**:
  - **Observation**: Video is captured by a USB camera at 60 frames per second
  - **Action**: Agent selected actions are sent to the console by one of the following.
    - A **mechanical actuator** (RoboTroller) that physically moves the CX40+ joystick
    - A **digital I/O module** that bypasses the joystick and sends signals directly to the controller port via the DB9 cable

This setup enables the study of RL algorithms in the physical world, in the presence of many real-world concerns (domain shifts, latency, and noise).

---

## System Setup

A complete hardware/software setup guide is available here:
[**System Setup**](docs/setup.md)

---

## Components

| Component           | Description |
|---------------------|-------------|
| **Console**         | [Atari 2600+](https://www.amazon.com/Atari-2600/dp/B0CG7LMFKY) with CX40+ joystick |
| **Monitor**         | Any gaming monitor with native 16:9 resolution and 60Hz refresh rate |
| **Camera**          | [Razer Kiyo Pro (1080p)](https://www.amazon.com/dp/B08T1MWX6J) — supports 60FPS uncompressed |
| **Control (Option 1)** | Mechanical joystick control via servo-based actuator [RoboTroller](https://robotroller.keenagi.com) |
| **Control (Option 2)** | Digital I/O module — e.g., [MCC USB-1024LS](https://microdaq.com/usb-1024ls-24-bit-digital-input-output-i-o-module.php) |

> See [setup.md](docs/setup.md) for placement, lighting, USB bandwidth, tag positioning, and system setup.

---

## Design of the RL software interface

In the textbook picture, an RL agent interacts with its environment by exchanging signals for reward, observation, and action.  In the episodic domains there is also a signal for the end of episode. In most Atari/Gym interfaces this is extended to support signals for end-of-life, sequence truncation, and supporting a minimal action set in a game.  These additional signals are useful for accelerating early performance in Atari games, and ease of experimentation is a significant factor for the physical Atari platform. We have chosen to expose the additional signals to our learning agents.

We want the RL algorithms to have an interface that supports real-time interaction with the real world.  We have changed the agent/environment calling conventions a common choice (where the agent directly calls the environment) to an interface where the experiment infrastructure sends the signals to the agent and to the environment.

The primary agent/environment interface operates at 60fps.  Observations are received from the video camera (with some internal buffers) and sent to the agent.  Actions selected by the agent are converted into commands for the robotroller to move the joystick (which also has latencies).  More effort is required to extract signals for rewards, lives, and the end of episode from the observed video, and these are described below.

## Games

We restricted our attention to console games that only require a **fire button press** to start gameplay.  Many Atari games require toggling the physical **reset switch** on the Atari console to restart the game, and so were not used.


The following games are known to work with this setup and cover a range of visual styles and control demands:

_Recommended_

- **Ms. Pac-Man**
- **Centipede**
- **Up 'n Down**
- **Krull**

_Less Tested_
- **Q*Bert**
- **Battle Zone**
- **Atlantis**
- **Defender**

## Detecting Score, Lives, and the End of Game


Detection of the game score, lives, and the end of game requires custom logic for the Physical Atari.  In the ALE, bits from the internal game state are used to compute these signals in a custom manner for each game.  For the physical Atari, these signals must be computed from the video screen.  Multiple steps are required for extracting these signals, and it is the most brittle part of the physical atari platform.

For the first step, camera images are rectified by identifying the corners of the Atari game screen in the camera image and applying a standard linear transformation.  We have tried multiple approaches here. One reliable approach is to manually identify the four screen corners, as these do not vary by Atari game.  This approach breaks down if the system is subject to jostling or vibration.  Two other approaches we have examined are the use of April tags, and whole screen recognition.

For the second step we manually identify boxes around the score and lives for each game.

For the third step, the score is read from the video.  The digits used in each atari game differs substantially, and they are also distinct from digits in standard online datasets such as MNIST.  We again tried multiple approaches.  The most reliable was to collect a dataset for each game of images and numerical scores, and then train a supervised learning classifier for each.  For training the classifier, the captured images are augmented with several transformations, to account for small variations in calibration, lighting, and geometry.  A similar process is used to detect the number of lives in the game.

Some additional custom logic is used on top of the neural net classifiers to extract reliable signals.  Score differences are validated for consistency with the game scores observed in the simulator, so scores can't change by an unrealizable amount between frames.  Additional logic is present to recover from transient errors, and to detect a plausible end of the game.  When the game is presumed to be over, a FIRE action is sent to restart, and the end of game is sent to the learning agent.

// A per-game CRNN model is used to extract the score directly from screen pixels. These models are trained on ALE-rendered frames, using known score regions for supervision.

// For some games, a more targeted model may be used instead.

### Additional Considerations for ROM and Hardware Variability

Several additional ROM and hardware factors can impact signal extraction accuracy. Different ROM revisions may introduce subtle or significant variations, including difficulty adjustments, bug fixes, or changes in visual indicators such as the displayed number of lives. Region-specific differences between NTSC and PAL cartridges could also potentially result in rendering variations, timing discrepancies, or different memory layouts, although the extent to which these factors necessitate separate processing pipelines remains to be fully tested.

Additionally, prototype versions of games may differ notably from retail releases, leading to mismatches if the ALE emulator is based on a retail ROM and the physical cartridge represents an earlier or alternative version. Furthermore, variations in bankswitching techniques, used by certain cartridges to extend memory addressing, could also explain inconsistencies between the simulated ALE environment and physical hardware, particularly if RAM addresses differ for critical indicators like lives or score. These factors collectively underscore the importance of careful validation and customization when transitioning from simulation to the physical Atari platform.

---

## Research Challenges

This project focuses on bridging the **reality gap** in empirical reinforcement learning research.

**Differences between the emulator and reality include:**
- No turn-taking in a real-time environment
- Visual statistics (emulator vs real camera)
- Latency in video capture and actuator response
- Imperfect lighting, reflections, and image noise
- Score detection errors under variable lighting and resolution

Both trained policies and reinforcement learning algorithms can degrade significantly when exposed to these real-world conditions, even if they perform well in simulation.

---

## Launching

To run the physical setup, build the docker environment with:

```
./docker_build.sh
```

Run the docker environment with:

```
./docker_run.sh
```

Launch the physical harness:

```
python physical_harness.py
```

An example run for the Ms Pacman game, for a custom configuration

```
python3 harness_physical.py \
 --detection_config=configs/screen_detection/fixed.json \
 --game_config=configs/games/ms_pacman.json \
 --agent_type=agent_delay_target \
 --reduce_action_set=2 --gpu=0 \
 --joystick_config=configs/controllers/robotroller.json \
 --total_frames=1_000_000
```

## Single-Game Streaming Benchmark Runner

This repository also includes a minimal Atari benchmark harness that uses `ale_py` directly (no Gym wrappers), with:
- streaming agent calls every frame
- runner-enforced frame skip
- runner-enforced action latency queue
- ALE sticky actions (`repeat_action_probability`)
- per-frame JSONL logging for reproducibility

### How to run

```
python -m benchmark.run_single_game \
  --game ms_pacman \
  --seed 0 \
  --frames 200000 \
  --frame-skip 4 \
  --delay 6 \
  --sticky 0.25 \
  --full-action-space 1 \
  --logdir ./runs
```

Outputs are written to a timestamped run directory:
- `config.json`
- `events.jsonl` (one row per frame)
- `episodes.jsonl` (one row per episode end)

### How to validate delay/frame-skip mechanics

Run the mechanics tests:

```
pytest -q benchmark/tests/test_mechanics.py
```

These tests verify:
- delay queue correctness
- frame-skip decision boundaries
- combined delay + frame-skip behavior
- logging completeness
- queue reinitialization and episode counter behavior across resets

### Carmack-Compatible Single-Run Profile

The single-game runner also supports a Carmack-compatible profile:

```bash
python -m benchmark.run_single_game \
  --runner-mode carmack_compat \
  --frame-skip 1 \
  --game breakout \
  --agent delay_target \
  --frames 200000 \
  --delay 0 \
  --sticky 0.0 \
  --full-action-space 0 \
  --life-loss-termination 0 \
  --lives-as-episodes 1 \
  --max-frames-without-reward 18000 \
  --reset-on-life-loss 0 \
  --logdir ./runs/single_carmack
```

Key contract points:
- agent-owned cadence (`--frame-skip 1` enforced)
- agent-facing boundary payload: `terminated`, `truncated`, `end_of_episode_pulse`
- `boundary_cause` remains log-only (not agent-facing)
- schema/profile tags on config/events/episodes:
  - `single_run_profile="carmack_compat"`
  - `single_run_schema_version="carmack_single_v1"`

Validate a run directory with:

```bash
python -m benchmark.validate_contract \
  --run-dir ./runs/single_carmack/<run_id> \
  --sample-event-lines 10
```

The frozen contract spec is in:
- `benchmark/contracts/carmack_single_v1_contract.md`

## Multi-Game Continual Benchmark Runner (v1)

The v1 runner extends v0 into a single continual stream over scheduled game visits, still using direct `ale_py` / `ALEInterface` calls (no Gym wrappers). Key semantics:
- agent is called every frame (streaming)
- runner enforces decision interval (`--decision-interval`, action repeat) and delay queue (action latency)
- game switches are logged as `truncated=True` on the final frame of each visit
- environment terminals remain `terminated=True` (distinct from truncation)
- `episode_id` increments only on true terminals (`terminated=True`)
- `segment_id` increments on any boundary (`terminated` or `truncated`)
- delay queue and decision phase are reset at environment boundaries
- anti-leak scheduling: per-seed randomized cycle order + jittered visit lengths; schedule identity is not passed in agent `info`

The action policy is global-to-local mapped:
- agent outputs an index in a fixed global ALE action space (typically standard 0..17)
- runner maps that ALE action to the current game's local action set
- if action is illegal for current game, runner falls back to the configured default action (or local index 0)

### How to run (v1)

```bash
python -m benchmark.run_multigame \
  --games ms_pacman,centipede,qbert,defender,krull,atlantis,up_n_down,battle_zone \
  --num-cycles 3 \
  --base-visit-frames 200000 \
  --jitter-pct 0.07 \
  --min-visit-frames 1 \
  --seed 0 \
  --decision-interval 4 \
  --delay 6 \
  --sticky 0.25 \
  --full-action-space 1 \
  --logdir ./runs/v1
```

You can also run from a committed JSON config (recommended for reproducibility):

```bash
python -m benchmark.run_multigame \
  --config configs/v1_reference.json \
  --seed 0 \
  --logdir ./runs/v1_ref
```

CLI flags still override config file values, for example:

```bash
python -m benchmark.run_multigame \
  --config configs/v1_smoke.json \
  --seed 3 \
  --delay 4 \
  --logdir ./runs/v1_smoke_override
```

Outputs are written to a timestamped run directory:
- `config.json` (full config, software versions, realized schedule, per-game action sets, mapping policy)
- `events.jsonl` (one row per frame, including game/visit/cycle indices, `episode_id`, `segment_id`, and both terminal signals)
- `episodes.jsonl` (one row per true terminal only, `ended_by=\"terminated\"`)
- `segments.jsonl` (one row per reset boundary, `ended_by in {\"terminated\",\"truncated\"}`)

### TinyDQN baseline agent

`--agent tinydqn` enables a compact online-learning baseline (streaming DQN):
- called every frame, but only chooses new actions on decision frames
- trains from a small replay buffer on decision frames
- treats both `terminated` and `truncated` as bootstrap-done for replay targets
- intended for calibration and sanity checks, not SOTA performance

Example:

```bash
python -m benchmark.run_multigame \
  --games pong,breakout,ms_pacman,centipede \
  --num-cycles 3 \
  --base-visit-frames 10000 \
  --jitter-pct 0.05 \
  --min-visit-frames 600 \
  --seed 0 \
  --decision-interval 4 \
  --delay 2 \
  --sticky 0.25 \
  --full-action-space 1 \
  --life-loss-termination 1 \
  --agent tinydqn \
  --dqn-gamma 0.99 \
  --dqn-lr 1e-4 \
  --dqn-buffer-size 10000 \
  --dqn-batch-size 32 \
  --dqn-train-every 4 \
  --dqn-target-update 250 \
  --dqn-eps-start 1.0 \
  --dqn-eps-end 0.05 \
  --dqn-eps-decay-frames 200000 \
  --dqn-replay-min 1000 \
  --dqn-device cpu \
  --logdir ./runs/baseline_tinydqn
```

TinyDQN hyperparameters are saved under `agent_config` in the run `config.json`.

### How to validate multi-game mechanics

```bash
pytest -q benchmark/tests/test_mechanics.py benchmark/tests/test_multigame.py
```

The v1 tests verify:
- deterministic schedule materialization from seed/config
- truncation only at visit boundaries
- anti-leak agent info payload (no schedule/boundary counters)
- `episode_id` increments only on terminated frames
- `segment_id` increments on terminals and truncations
- `episodes.jsonl` contains only terminated episodes
- `segments.jsonl` contains both truncation and termination boundaries
- delay queue reset on switches
- decision-interval cadence behavior

## Run Scoring and Analysis

Use `benchmark.score_run` to compute benchmark metrics from an existing run directory (no ALE/ROM dependency):

```bash
python -m benchmark.score_run \
  --run-dir ./runs/v1/<run_dir> \
  --window-episodes 20 \
  --bottom-k-frac 0.25 \
  --revisit-episodes 5
```

This writes `score.json` into the run directory and prints the same JSON to stdout.

### Metric Definitions

- Online deployment score (primary):
  - per game, compute mean return over recent episodes in the last cycle
  - aggregate with both average-game performance and bottom-k robustness:
    - `mean_score = mean(per_game_scores)`
    - `bottom_k_score = mean(worst ceil(k_frac * N) per_game_scores)`
    - default `final_score = 0.5 * mean_score + 0.5 * bottom_k_score`
- Forgetting index (diagnostic):
  - for revisits of each game, compare end of previous visit vs start of next revisit
  - drop = pre - post; higher positive values indicate more forgetting
- Plasticity index (diagnostic):
  - compares early vs late performance inside the first cycle
  - higher values indicate faster early adaptation
- Runtime stats:
  - FPS from `events.jsonl` wallclock timestamps when available
  - `notes.fps_source` indicates whether frame count came from events or config fallback

### Terminated vs Truncated

- `terminated` means a true environment terminal (e.g., game over / life-loss policy).
- `truncated` means an external benchmark boundary (time budget / scheduled game switch), not an MDP terminal.

Scoring uses this distinction directly:
- `episodes.jsonl` contributes true-episode metrics (`terminated` only).
- `segments.jsonl` contributes boundary-aware metrics (both terminated and truncated).
- `notes.unassigned_episode_count` reports episodes that could not be safely placed into any visit window.

## Reference Config and Calibration Suite

The repository includes canonical benchmark configs:
- `configs/v1_reference.json`: long-form reference setup (8 games, 3 cycles, sticky actions, delay, decision interval).
- `configs/v1_smoke.json`: fast validation setup for mechanics and scoring smoke checks.

Why these defaults matter:
- `sticky=0.25` keeps ALE stochastic and avoids determinism exploits.
- `decision_interval=4` models repeated actions between control updates.
- `delay>0` enforces explicit actuation latency in the runner.
- visit-end `truncated` flags keep benchmark switches distinct from true environment `terminated` events.

Run the calibration suite:

```bash
python -m benchmark.calibrate --suite smoke --out runs/calib_smoke
python -m benchmark.calibrate --suite calib --out runs/calib_medium
python -m benchmark.calibrate --suite paper --out runs/calib_paper
```

Suite behavior:
- Runs `RepeatActionAgent`, `RandomAgent`, and `TinyDQNAgent` (TinyDQN skipped if `torch` is unavailable).
- Scores each run with `benchmark.score_run`, writing per-run `score.json`.
- Writes aggregate `summary.json` under the suite output directory with per-agent stats.
- `smoke` enforces lightweight expectations and exits non-zero on failures (mechanics/logging checks + robust ordering checks).

`summary.json` reports:
- per-agent score stats (`final_score`, `mean_score`, `bottom_k_score`: mean/median/std/min/max/CV),
- runtime (`fps` and `frames` means),
- forgetting/plasticity aggregates when available,
- skipped/failed run counts and smoke expectation results.


---

## System Performance and Profiling

Running the physical setup reliably requires that the system meets strict performance requirements — especially with regard to CPU and GPU power settings, thermal limits, and scheduling behavior.

Modern systems often default to power-saving configurations that can cause unexpected latency, frame delays, or jitter. These issues are especially problematic in real-time or hardware-in-the-loop setups.

- See [Performance Setup](./docs/setup.md#system-performance-validation) for validating system configuration and fixing system-level performance issues.
- See [Profiling Guide](./docs/profiling.md) for details on collecting and analyzing performance data using NVIDIA Nsight Systems and NVTX annotations.

---

## License

This project is licensed under the Apache 2.0 License.

Unless otherwise noted, this license applies to all source code and pre-trained model files included in the repository.
